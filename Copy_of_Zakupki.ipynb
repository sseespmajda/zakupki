{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajDyGemoV8Q"
      },
      "source": [
        "# Zakupki website scraping for Piotr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfVriCvQPi8u"
      },
      "source": [
        "The aim of this notebook is to scrape details of each contract hosted on the Russian Zakupki public sector contract awarding website.\n",
        "\n",
        "The input for this project will be the Zakupki URL. This code can be run on different dates to pull fresh contract data.\n",
        "\n",
        "Method:\n",
        "1.   Identify the number of pages of contracts to be scraped (using the contract filters provided).\n",
        "2.   Iterate through each page, scraping the registration number of each contract.\n",
        "3.   Access the website for each contract by placing the registraion number in the URL.\n",
        "4.   Scrape the details for each contract and add them to a list of Contracts dataclasses.\n",
        "5.   Format these Contract objects as a dataframe and output the dataframe to a csv file.\n",
        "\n",
        "\n",
        "The output of this project will be the CSV file, with each row representing a new contract from the webstie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O34U-48RLCa"
      },
      "source": [
        "### Section 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkywDpTl3_lN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import date\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "from dateutil import parser\n",
        "from threading import Thread\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import date, timedelta\n",
        "import logging\n",
        "import http.client\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohoWQEyyCpkk"
      },
      "outputs": [],
      "source": [
        "logging = False\n",
        "\n",
        "if logging:\n",
        "\n",
        "    http.client.HTTPConnection.debuglevel = 1\n",
        "\n",
        "    # You must initialize logging, otherwise you'll not see debug output.\n",
        "    logging.basicConfig()\n",
        "    logging.getLogger().setLevel(logging.DEBUG)\n",
        "    requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
        "    requests_log.setLevel(logging.DEBUG)\n",
        "    requests_log.propagate = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJWBR30Apf9y"
      },
      "source": [
        "### Section 2: Determine Number of pages to scrape\n",
        "Test connection to the website and determine number of pages to scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqetG6kV0HFi",
        "outputId": "d351bc31-6ef2-4d6c-89a8-bf9234b503dc"
      },
      "outputs": [],
      "source": [
        "# Getting the dates we want to scrape.\n",
        "\n",
        "\n",
        "url=\"https://zakupki.gov.ru/epz/contract/search/results.html?morphology=on&search-filter=%D0%94%D0%B0%D1%82%D0%B5+%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F&fz44=on&contractStageList_0=on&contractStageList_1=on&contractStageList=0%2C1&contractPriceFrom=1000000&contractCurrencyID=-1&budgetLevelsIdNameHidden=%7B%7D&customerPlace=5277383&customerPlaceCodes=66000000000&contractDateFrom={}&contractDateTo={}&selectedLaws=FZ44&sortBy=UPDATE_DATE&pageNumber={}&sortDirection=false&recordsPerPage=_10&showLotsInfoHidden=false\"\n",
        "start = date(2016, 1, 1)\n",
        "end = date(2021, 12, 31)\n",
        "days = timedelta(days=1)\n",
        "\n",
        "calendar=[]\n",
        "\n",
        "### this actually works, set start & end date, it creates search results urls for each days with some criteria to limit the number (10 mln RUB, finished)\n",
        "while start<=end:\n",
        "  # tempURL=url.format(start.strftime('%d.%m.%Y'),start.strftime('%d.%m.%Y'))\n",
        "  calendar.append(start.strftime('%d.%m.%Y'))\n",
        "  start+=days\n",
        "\n",
        "print(calendar)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Ol6Rbipi4N"
      },
      "source": [
        "\n",
        "### Section 3: Scrape each registration number\n",
        "\n",
        "Scrape the reg numbers of each contract, so they can be accessed individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxF2lQm5Vq_8"
      },
      "outputs": [],
      "source": [
        "def getPage(tempURL):\n",
        "  # If User-Agent is not set to custom, the website will know a Python script is accessing it and block some of the request\n",
        "  return BeautifulSoup(requests.get(tempURL, headers={'User-Agent': 'Custom'}).content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Page:\n",
        "\n",
        "    def __init__(self, day, pageNum, pagefile):\n",
        "\n",
        "        self.day = day\n",
        "        self.pageNum = pageNum\n",
        "        self.pagefile = pagefile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seJyyDBCFLGl"
      },
      "outputs": [],
      "source": [
        "# Getting the web page for all the contracts for each date in the range we want to scrape.\n",
        "\n",
        "\n",
        "pages = []\n",
        "\n",
        "\n",
        "for day in calendar:\n",
        "\n",
        "  tempURL = url.format(1, day, day)\n",
        "\n",
        "  page = getPage(tempURL)\n",
        "\n",
        "  # Scrape the max number of pages\n",
        "  try:\n",
        "    maxPageNum = int(page.select('a[data-pagenumber]')[-2].find(\"span\").text)\n",
        "  except:\n",
        "    maxPageNum = 1\n",
        "\n",
        "  print(maxPageNum)\n",
        "    \n",
        "  for i in range(1, maxPageNum+1):\n",
        "\n",
        "    contracts = []\n",
        "\n",
        "    # Creating a temporary URL for each page containing contracts\n",
        "    tempPageURL = url.format(i, day, day)\n",
        "    print(tempPageURL)\n",
        "\n",
        "    # Request the page and format it as a BeautifulSoup object so that we can perform scrapings\n",
        "    page = getPage(tempPageURL)\n",
        "\n",
        "    pages.append(Page(day, i, page))\n",
        "\n",
        "    # print(page)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "print(str(len(pages)), \" pages found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regNumbers = []\n",
        "\n",
        "\n",
        "for page in pages:\n",
        "\n",
        "\n",
        "  # Obtain a list of all the sections of HTML containing a contract in the web page\n",
        "  listOfContracts = page.pagefile.find_all(\"div\", {\"class\": \"registry-entry__header-mid__number\"})\n",
        "  \n",
        "\n",
        "  # Segment the registration number from the URL of each contract\n",
        "  for contract in listOfContracts:\n",
        "    href = contract.find(\"a\")['href']\n",
        "    regNum = href[href.index(\"Number=\")+7:]\n",
        "    regNumbers.append(regNum)\n",
        "\n",
        "  # print(regNumbers)\n",
        "\n",
        "  print(\"done\", i, len(listOfContracts), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFa1lj9sHpkK",
        "outputId": "32168ad0-7959-428c-9ad0-8d707a6f732a"
      },
      "outputs": [],
      "source": [
        "url=\"https://zakupki.gov.ru/epz/contract/contractCard/common-info.html?reestrNumber=2665807811020000160\"\n",
        "soup=getPage(url)\n",
        "table=soup.findAll(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "org=table[0].text.strip().split(\"\\n\")\n",
        "procurer=org[0]\n",
        "INN=org[9]\n",
        "KPP=org[13]\n",
        "registered=org[17]\n",
        "\n",
        "address=table[2].text.strip()\n",
        "\n",
        "numbermail=table[4].text.strip().split(\"\\n\")\n",
        "number=numbermail[0]\n",
        "mail=numbermail[1].strip()\n",
        "\n",
        "#productCode=table[1].text.strip()\n",
        "# #Contracted=tables[3].find_all(\"td\",{\"class\":\"tableBlock__col\"})  ### that's about finding the winner and winning bid, the real of the contract\n",
        "# #Supplier=Contracted[2].text # WORKS\n",
        "# #FinalPrice=Contracted[3].text  # works\n",
        "#print(table[0])\n",
        "print(number)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L254l5YhpqNv"
      },
      "source": [
        "### Section 4: Details scraping\n",
        "\n",
        "The Contract Dataclass will store the information during scraping.\n",
        "If any information can't be scraped, default values have been provided in their place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg5Td-OGi3vz"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Contract:\n",
        "\n",
        "  # TODO: Add reg number to class\n",
        "\n",
        "  # Main Section\n",
        "  ID: str = \"none\"\n",
        "  price: float = 0.0\n",
        "  published: date = None\n",
        "  deadline: date = None\n",
        "  # organisationName: str = \"none\"\n",
        "\n",
        "  # Tab 1\n",
        "  method: str = \"none\"\n",
        "  tenderObject: str = \"none\"\n",
        "  organisationName: str = \"none\"\n",
        "  address: str = \"none\"\n",
        "  official: str = \"none\"\n",
        "  productCode: str = \"none\"\n",
        "\n",
        "\n",
        "  # Tab 2\n",
        "  # We may be missing info for this tab\n",
        "  participants: str = \"none\"\n",
        "  bids: float = 0.0\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yatvXkvpz2H"
      },
      "source": [
        "Method for scraping the data from each contract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq9BLFkUaDPW"
      },
      "outputs": [],
      "source": [
        "def scrapeData(reg):\n",
        "\n",
        "  dir = \"https://zakupki.gov.ru/epz/order/notice/ea44/view/{}.html?regNumber={}\"\n",
        "\n",
        "\n",
        "  # TODO Add function to retry contracts that have their connection aborted.\n",
        "\n",
        "\n",
        "  # Getting the web page for the given contract\n",
        "  tempDir = dir.format(\"common-info\", reg)\n",
        "  print(tempDir)\n",
        "  page = getPage(tempDir)\n",
        "\n",
        "  # print(page)\n",
        "\n",
        "  ### Basic info - Unique ID, Initial price, publication date and deadline \n",
        "\n",
        "  # ID = page.find(\"span\",{\"class\":\"navBreadcrumb__text\"}).text.strip()  ## Returns unique ID with No at the start from each particular tender page (not the results list)\n",
        "  ID = reg\n",
        "  try:\n",
        "    price=page.find(\"span\", {\"class\":\"cardMainInfo__content cost\"}).text.replace(\"\\xa0\",\",\").replace(\" \",\"\").replace(\"â‚½\",\"\").strip()\n",
        "  except:\n",
        "    price=0.0\n",
        "  cardInfo=page.find_all(\"span\",{\"class\":\"cardMainInfo__content\"}) ## there's list inside, with procurer and price if needed\n",
        "  published=str(cardInfo[3].text).strip()\n",
        "  deadline=str(cardInfo[4].text).strip()\n",
        "\n",
        "  ### First part info - tender type, object, procurer, address and official in charge\n",
        "\n",
        "  sectionInfo=page.find_all(\"span\",{\"class\":\"section__info\"})       ## same issue, but we can get a lot of stuff from this tab\n",
        "  method=sectionInfo[0].text\n",
        "  tenderObject=sectionInfo[4].text\n",
        "  organisationName=sectionInfo[8].text                              ## e.g. like that?\n",
        "  address=str(sectionInfo[9].text).strip()                                       ## 6 and 7 positions on the list return the same data..?\n",
        "  official=str(sectionInfo[11].text).strip()                                      ## just in case, price can be also extracted from here, nr. 13\n",
        "\n",
        "  ### Data taken from purchase order, it might contain multiple codes, to be tested! \n",
        "  tableBlock=page.find_all(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "  productCode=tableBlock[1].text.strip()\n",
        "\n",
        "  ### Second part info - participants and bids\n",
        "\n",
        "\n",
        "  tempDir = dir.format(\"supplier-results\", reg)\n",
        "  print(tempDir)\n",
        "  page = getPage(tempDir)\n",
        "\n",
        "  tableBlock2=page.find_all(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "  participants=tableBlock2[2].text\n",
        "  bids=tableBlock2[3].text\n",
        "\n",
        "  print(participants, bids)\n",
        "\n",
        "  # Create the Contract dataclass object and append it to a list of objects.\n",
        "  # This method means that missing data can be accounted for.\n",
        "  contract = Contract(ID, price, published, deadline, method, tenderObject, organisationName, address, official, productCode)\n",
        "  contracts.append(contract)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CitfSwF2p94x"
      },
      "source": [
        "### Section 5: Starting execution\n",
        "Scrape the contracts themselves using threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLF0rfm4uOgY"
      },
      "outputs": [],
      "source": [
        "### this soup is used later on to extract values from tender page\n",
        "#soup1 = BeautifulSoup(requests.get(url1, headers={'User-Agent': 'Custom'}).content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RseKcK3_XT7R",
        "outputId": "2e193036-25a2-4b02-8d91-34a6293d9141"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "contracts = []\n",
        "\n",
        "\n",
        "print(\"Starting scrape with {} reg numbers\".format(len(regNumbers)))\n",
        "\n",
        "# Ensuring that hte code works without threading first\n",
        "scrapeData(regNumbers[0])\n",
        "\n",
        "# print(contracts)\n",
        "\n",
        "# for regNum in tqdm(regNumbers):\n",
        "#     scrapeData(regNum)\n",
        "\n",
        "\n",
        "# for regNumber in tqdm(regNumbers[:50]):\n",
        "# # #  scrapeData(regNumber)\n",
        "#   thread = Thread(target = scrapeData, args = (regNumber,))\n",
        "#   thread.start()\n",
        "\n",
        "# print(contracts)\n",
        "\n",
        "\n",
        "# For each contract reg number, start a thread for its execution and display the progress.\n",
        "# for reg in tqdm(regNumbers):\n",
        "#   thread = Thread(target = scrapeData, args = (reg,))\n",
        "#   thread.start()\n",
        "\n",
        "\n",
        "# The contracts list now contains only successfully scraped contracts\n",
        "#print(len(contracts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHKuKTFqLij"
      },
      "source": [
        "Usually about 9/10 contracts are scraped successfully. This depends on how the Zakupki website is feeling that day. Sometimes erroneous webpages are returned instead of the webpage for the requested contract."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9_G7hUrqT6g"
      },
      "source": [
        "### Section 6: Output\n",
        "\n",
        "Convert the list of contract classes to a dataframe so that they can be exported to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6p3oMf-eJbP"
      },
      "outputs": [],
      "source": [
        "contract = contracts[0]\n",
        "print(contract.__dict__.keys())\n",
        "df = pd.DataFrame(columns=contract.__dict__.keys())\n",
        "\n",
        "for contract in contracts:\n",
        "  df = df.append(contract.__dict__, ignore_index=True)\n",
        "\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ST7Qk4rVOFK"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"zakupki.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Zakupki.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
