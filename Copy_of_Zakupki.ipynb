{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajDyGemoV8Q"
      },
      "source": [
        "# Zakupki website scraping for Piotr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfVriCvQPi8u"
      },
      "source": [
        "The aim of this notebook is to scrape details of each contract hosted on the Russian Zakupki public sector contract awarding website.\n",
        "\n",
        "The input for this project will be the Zakupki URL. This code can be run on different dates to pull fresh contract data.\n",
        "\n",
        "Method:\n",
        "1.   Identify the number of pages of contracts to be scraped (using the contract filters provided).\n",
        "2.   Iterate through each page, scraping the registration number of each contract.\n",
        "3.   Access the website for each contract by placing the registraion number in the URL.\n",
        "4.   Scrape the details for each contract and add them to a list of Contracts dataclasses.\n",
        "5.   Format these Contract objects as a dataframe and output the dataframe to a csv file.\n",
        "\n",
        "\n",
        "The output of this project will be the CSV file, with each row representing a new contract from the webstie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O34U-48RLCa"
      },
      "source": [
        "### Section 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "EkywDpTl3_lN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import date\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "from dateutil import parser\n",
        "from threading import Thread\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import date, timedelta\n",
        "import logging\n",
        "import http.client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "ohoWQEyyCpkk"
      },
      "outputs": [],
      "source": [
        "logging = False\n",
        "\n",
        "if logging:\n",
        "\n",
        "    http.client.HTTPConnection.debuglevel = 1\n",
        "\n",
        "    # You must initialize logging, otherwise you'll not see debug output.\n",
        "    logging.basicConfig()\n",
        "    logging.getLogger().setLevel(logging.DEBUG)\n",
        "    requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
        "    requests_log.setLevel(logging.DEBUG)\n",
        "    requests_log.propagate = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJWBR30Apf9y"
      },
      "source": [
        "### Section 2: Determine Number of pages to scrape\n",
        "Test connection to the website and determine number of pages to scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqetG6kV0HFi",
        "outputId": "d351bc31-6ef2-4d6c-89a8-bf9234b503dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['01.06.2016']\n"
          ]
        }
      ],
      "source": [
        "# Getting the dates we want to scrape.\n",
        "\n",
        "url=\"https://zakupki.gov.ru/epz/contract/search/results.html?morphology=on&search-filter=%D0%94%D0%B0%D1%82%D0%B5+%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F&fz44=on&contractStageList_0=on&contractStageList_1=on&contractStageList=0%2C1&contractPriceFrom=1000000&contractCurrencyID=-1&budgetLevelsIdNameHidden=%7B%7D&customerPlace=5277383&customerPlaceCodes=66000000000&contractDateFrom={}&contractDateTo={}&selectedLaws=FZ44&sortBy=UPDATE_DATE&pageNumber={}&sortDirection=false&recordsPerPage=_500&showLotsInfoHidden=false\"\n",
        "start = date(2016, 6, 1)\n",
        "end = date(2016, 6, 1)\n",
        "days = timedelta(days=1)\n",
        "\n",
        "calendar=[]\n",
        "\n",
        "### this actually works, set start & end date, it creates search results urls for each days with some criteria to limit the number (10 mln RUB, finished)\n",
        "while start<=end:\n",
        "  # tempURL=url.format(start.strftime('%d.%m.%Y'),start.strftime('%d.%m.%Y'))\n",
        "  calendar.append(start.strftime('%d.%m.%Y'))\n",
        "  start+=days\n",
        "\n",
        "print(calendar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Ol6Rbipi4N"
      },
      "source": [
        "\n",
        "### Section 3: Scrape each registration number\n",
        "\n",
        "Scrape the reg numbers of each contract, so they can be accessed individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "IxF2lQm5Vq_8"
      },
      "outputs": [],
      "source": [
        "def getPage(tempURL):\n",
        "  # If User-Agent is not set to custom, the website will know a Python script is accessing it and block some of the request\n",
        "  return BeautifulSoup(requests.get(tempURL, headers={'User-Agent': 'Custom'}).content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Page:\n",
        "\n",
        "    def __init__(self, day, pageNum, pagefile):\n",
        "\n",
        "        self.day = day\n",
        "        self.pageNum = pageNum\n",
        "        self.pagefile = pagefile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "seJyyDBCFLGl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "https://zakupki.gov.ru/epz/contract/search/results.html?morphology=on&search-filter=%D0%94%D0%B0%D1%82%D0%B5+%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F&fz44=on&contractStageList_0=on&contractStageList_1=on&contractStageList=0%2C1&contractPriceFrom=1000000&contractCurrencyID=-1&budgetLevelsIdNameHidden=%7B%7D&customerPlace=5277383&customerPlaceCodes=66000000000&contractDateFrom=01.06.2016&contractDateTo=01.06.2016&selectedLaws=FZ44&sortBy=UPDATE_DATE&pageNumber=1&sortDirection=false&recordsPerPage=_500&showLotsInfoHidden=false\n",
            "1  pages found\n"
          ]
        }
      ],
      "source": [
        "# Getting the web page for all the contracts for each date in the range we want to scrape.\n",
        "\n",
        "pages = []\n",
        "\n",
        "for day in calendar:\n",
        "\n",
        "  tempURL = url.format(day, day, 1)\n",
        "\n",
        "  page = getPage(tempURL)\n",
        "\n",
        "  # Scrape the max number of pages\n",
        "  try:\n",
        "    maxPageNum = int(page.select('a[data-pagenumber]')[-2].find(\"span\").text)\n",
        "  except:\n",
        "    maxPageNum = 1\n",
        "\n",
        "  print(maxPageNum)\n",
        "    \n",
        "  for i in range(1, maxPageNum+1):\n",
        "\n",
        "    contracts = []\n",
        "\n",
        "    # Creating a temporary URL for each page containing contracts\n",
        "    tempPageURL = url.format(day, day, i)\n",
        "    print(tempPageURL)\n",
        "\n",
        "    # Request the page and format it as a BeautifulSoup object so that we can perform scrapings\n",
        "    page = getPage(tempPageURL)\n",
        "\n",
        "    pages.append(Page(day, i, page))\n",
        "\n",
        "    # print(page)\n",
        "\n",
        "print(str(len(pages)), \" pages found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done 1 40 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "regNumbers = []\n",
        "\n",
        "\n",
        "for page in pages:\n",
        "\n",
        "\n",
        "  # Obtain a list of all the sections of HTML containing a contract in the web page\n",
        "  listOfContracts = page.pagefile.find_all(\"div\", {\"class\": \"registry-entry__header-mid__number\"})\n",
        "  \n",
        "\n",
        "  # Segment the registration number from the URL of each contract\n",
        "  for contract in listOfContracts:\n",
        "    href = contract.find(\"a\")['href']\n",
        "    regNum = href[href.index(\"Number=\")+7:]\n",
        "    regNumbers.append(regNum)\n",
        "\n",
        "  # print(regNumbers)\n",
        "\n",
        "  print(\"done\", i, len(listOfContracts), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below is just for testing labels etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFa1lj9sHpkK",
        "outputId": "32168ad0-7959-428c-9ad0-8d707a6f732a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07.09.2020\n"
          ]
        }
      ],
      "source": [
        "url=\"https://zakupki.gov.ru/epz/contract/contractCard/common-info.html?reestrNumber=2665807811020000160\"\n",
        "soup=getPage(url)\n",
        "# products=page.findAll(\"span\",{\"class\":\"section__info\"})\n",
        "# product=products[25].text.strip()\n",
        "# print(product)\n",
        "\n",
        "### first tab info ###\n",
        "table=soup.findAll(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "cardinfo=soup.findAll(\"span\",{\"class\":\"cardMainInfo__content\"})\n",
        "signed=cardinfo[3].text.strip()\n",
        "deadline=cardinfo[4].text.strip()\n",
        "# # price=soup.find(\"span\", {\"class\":\"cardMainInfo__content cost\"}).text.replace(\"\\xa0\",\"\").replace(\",\",\".\").replace(\"₽\",\"\").strip()\n",
        "print(signed)\n",
        "# # org=table[0].text.strip().split(\"\\n\")\n",
        "# # procurer=org[0]\n",
        "# # INN=org[9]\n",
        "# # KPP=org[13]\n",
        "# # registered=org[17]\n",
        "\n",
        "# # address=table[2].text.strip()\n",
        "# # numbermail=table[4].text.strip().split(\"\\n\")\n",
        "# # number=numbermail[0]\n",
        "# # mail=numbermail[1].strip()\n",
        "# # section=soup.findAll(\"span\",{\"class\":\"section__info\"})\n",
        "# # method=section[6].text.strip()\n",
        "# # ### second tab info ###\n",
        "\n",
        "# secondtab=soup.findAll(\"div\", {\"class\": \"col\"})\n",
        "# # print(secondtab)\n",
        "# tables = []\n",
        "# for tab in secondtab:\n",
        "#     tables.append(tab.findAll(\"tbody\"))\n",
        "\n",
        "# object=tables[-1][0].findAll(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "# code=object[2].text.strip().split(\"\\n\")[-1].strip()\n",
        "# print(code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L254l5YhpqNv"
      },
      "source": [
        "### Section 4: Details scraping\n",
        "\n",
        "The Contract Dataclass will store the information during scraping.\n",
        "If any information can't be scraped, default values have been provided in their place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "Eg5Td-OGi3vz"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Contract:\n",
        "\n",
        "  # TODO: Add reg number to class\n",
        "\n",
        "  # Main Section\n",
        "  ID: str = \"none\"\n",
        "  price: float = 0.0\n",
        "  signed: date = None\n",
        "  deadline: date = None\n",
        "  # organisationName: str = \"none\"\n",
        "\n",
        "  # Tab 1\n",
        "  method: str = \"none\"\n",
        "  product: str = \"none\"\n",
        "  procurer: str = \"none\"\n",
        "  inn: str = \"none\"\n",
        "  kpp: str = \"none\"\n",
        "  registered: date = None\n",
        "  address: str = \"none\"\n",
        "  number: str = \"none\"\n",
        "  mail: str = \"none\"\n",
        "\n",
        "  # Tab 2\n",
        "  code: float = 0.0\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yatvXkvpz2H"
      },
      "source": [
        "Method for scraping the data from each contract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "Jq9BLFkUaDPW"
      },
      "outputs": [],
      "source": [
        "def scrapeData(reg):\n",
        "\n",
        "  dir = \"https://zakupki.gov.ru/epz/contract/contractCard/{}.html?reestrNumber={}\"\n",
        "\n",
        "  # TODO Add function to retry contracts that have their connection aborted.\n",
        "\n",
        "  # Getting the web page for the given contract\n",
        "  tempDir = dir.format(\"common-info\", reg)\n",
        "  print(tempDir)\n",
        "  page = getPage(tempDir)\n",
        "\n",
        "  ### Basic info - Unique ID, Initial price, publication date and deadline \n",
        "\n",
        "  # ID = page.find(\"span\",{\"class\":\"navBreadcrumb__text\"}).text.strip()  ## Returns unique ID with No at the start from each particular tender page (not the results list)\n",
        "  ID = reg\n",
        "\n",
        "  ### first tab info ###\n",
        "  table=page.findAll(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "  cardinfo=page.findAll(\"span\",{\"class\":\"cardMainInfo__content\"})\n",
        "  signed=cardinfo[3].text.strip()\n",
        "  deadline=cardinfo[4].text.strip()\n",
        "  price=page.find(\"span\", {\"class\":\"cardMainInfo__content cost\"}).text.replace(\"\\xa0\",\"\").replace(\",\",\".\").replace(\"₽\",\"\").strip()\n",
        "  section=page.findAll(\"span\",{\"class\":\"section__info\"})\n",
        "  method=section[6].text.strip()\n",
        "  products=page.findAll(\"span\",{\"class\":\"section__info\"})\n",
        "  product=products[25].text.strip()\n",
        "\n",
        "  ### details about winner - ALSO, THERE'S OPTION TO SCRAPE SUBCONTRACTORS ### \n",
        "  org=table[0].text.strip().split(\"\\n\")\n",
        "  procurer=org[0]\n",
        "  inn=org[9]\n",
        "  kpp=org[13]\n",
        "  registered=org[17]\n",
        "\n",
        "  ### contractor deets ### \n",
        "  address=table[2].text.strip()\n",
        "  numbermail=table[4].text.strip().split(\"\\n\")\n",
        "  number=numbermail[0]\n",
        "  mail=numbermail[1].strip()\n",
        "\n",
        "  ### second tab ###\n",
        "  tempDir = dir.format(\"payment-info-and-target-of-order\", reg)\n",
        "  print(tempDir)\n",
        "  page = getPage(tempDir)\n",
        "\n",
        "  ### code stands for the product code, which can be later identified to return industry type ### \n",
        "  secondtab=page.findAll(\"div\", {\"class\": \"col\"})\n",
        "  tables = []\n",
        "  for tab in secondtab:\n",
        "      tables.append(tab.findAll(\"tbody\"))\n",
        "  object=tables[-1][0].findAll(\"td\",{\"class\":\"tableBlock__col\"})\n",
        "  code=object[2].text.strip().split(\"\\n\")[-1].strip()\n",
        "\n",
        "  # Create the Contract dataclass object and append it to a list of objects.\n",
        "  # This method means that missing data can be accounted for.\n",
        "  contract = Contract(ID, price, signed, deadline, method, product, procurer, inn, kpp, registered, address, number, code)\n",
        "  contracts.append(contract)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CitfSwF2p94x"
      },
      "source": [
        "### Section 5: Starting execution\n",
        "Scrape the contracts themselves using threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RseKcK3_XT7R",
        "outputId": "2e193036-25a2-4b02-8d91-34a6293d9141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting scrape with 40 reg numbers\n",
            "https://zakupki.gov.ru/epz/contract/contractCard/common-info.html?reestrNumber=['3662902195116000023', '2665807811016000068', '3661901766717000002', '3661901766717000001', '1665800456616000013', '3660200986916000052', '1666100166016000053', '3663400781816000003', '1668600001016000017', '2666001041516000251', '2666001041516000249', '1666202396316000052', '3667319873216000019', '3660700596316000039', '1667033496216000260', '1667033496216000255', '2666001041516000256', '2666001041516000254', '2666001041516000253', '2666001041516000252', '3662305565816000016', '3660700596316000037', '3660700596316000036', '2666001041516000245', '2666001041516000257', '3660700596316000038', '3660700258516000045', '3660700596316000040', '3666403957416000005', '1666303221616000047', '2666001041516000255', '2666001041516000250', '2666001041516000248', '3663202282816000015', '2666001041516000246', '2666001041516000258', '2666001041516000244', '2666001041516000247', '2665500525816000017', '1667033496216000257']\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\majda\\Dropbox\\PhD\\Data\\github\\zakupki\\Copy_of_Zakupki.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000019?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting scrape with \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m reg numbers\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(regNumbers)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000019?line=6'>7</a>\u001b[0m \u001b[39m# Ensuring that hte code works without threading first\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000019?line=7'>8</a>\u001b[0m scrapeData(regNumbers)\n",
            "\u001b[1;32mc:\\Users\\majda\\Dropbox\\PhD\\Data\\github\\zakupki\\Copy_of_Zakupki.ipynb Cell 18'\u001b[0m in \u001b[0;36mscrapeData\u001b[1;34m(reg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000016?line=17'>18</a>\u001b[0m table\u001b[39m=\u001b[39mpage\u001b[39m.\u001b[39mfindAll(\u001b[39m\"\u001b[39m\u001b[39mtd\u001b[39m\u001b[39m\"\u001b[39m,{\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mtableBlock__col\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000016?line=18'>19</a>\u001b[0m cardinfo\u001b[39m=\u001b[39mpage\u001b[39m.\u001b[39mfindAll(\u001b[39m\"\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m\"\u001b[39m,{\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mcardMainInfo__content\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000016?line=19'>20</a>\u001b[0m signed\u001b[39m=\u001b[39mcardinfo[\u001b[39m3\u001b[39;49m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000016?line=20'>21</a>\u001b[0m deadline\u001b[39m=\u001b[39mcardinfo[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000016?line=21'>22</a>\u001b[0m price\u001b[39m=\u001b[39mpage\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mcardMainInfo__content cost\u001b[39m\u001b[39m\"\u001b[39m})\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\xa0\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m₽\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstrip()\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "contracts = []\n",
        "\n",
        "print(\"Starting scrape with {} reg numbers\".format(len(regNumbers)))\n",
        "\n",
        "# Ensuring that hte code works without threading first\n",
        "scrapeData(regNumbers) #it was regNumbers[]\n",
        "\n",
        "# # print(contracts)\n",
        "\n",
        "# for regNum in tqdm(regNumbers):\n",
        "#     scrapeData(regNum)\n",
        "\n",
        "\n",
        "# for regNumber in tqdm(regNumbers[:50]):\n",
        "# # #  scrapeData(regNumber)\n",
        "#   thread = Thread(target = scrapeData, args = (regNumber,))\n",
        "#   thread.start()\n",
        "\n",
        "# print(contracts)\n",
        "\n",
        "\n",
        "# # For each contract reg number, start a thread for its execution and display the progress.\n",
        "# for reg in tqdm(regNumbers):\n",
        "#   thread = Thread(target = scrapeData, args = (reg,))\n",
        "#   thread.start()\n",
        "\n",
        "\n",
        "# # The contracts list now contains only successfully scraped contracts\n",
        "# print(len(contracts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHKuKTFqLij"
      },
      "source": [
        "Usually about 9/10 contracts are scraped successfully. This depends on how the Zakupki website is feeling that day. Sometimes erroneous webpages are returned instead of the webpage for the requested contract."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9_G7hUrqT6g"
      },
      "source": [
        "### Section 6: Output\n",
        "\n",
        "Convert the list of contract classes to a dataframe so that they can be exported to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6p3oMf-eJbP"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\majda\\Dropbox\\PhD\\Data\\github\\zakupki\\Copy_of_Zakupki.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000022?line=0'>1</a>\u001b[0m contract \u001b[39m=\u001b[39m contracts[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000022?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(contract\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/majda/Dropbox/PhD/Data/github/zakupki/Copy_of_Zakupki.ipynb#ch0000022?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mcontract\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mkeys())\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "contract = contracts[0]\n",
        "print(contract.__dict__.keys())\n",
        "df = pd.DataFrame(columns=contract.__dict__.keys())\n",
        "\n",
        "for contract in contracts:\n",
        "  df = df.append(contract.__dict__, ignore_index=True)\n",
        "\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ST7Qk4rVOFK"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"zakupki.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Zakupki.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
