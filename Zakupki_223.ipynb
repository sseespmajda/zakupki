{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rajDyGemoV8Q"
   },
   "source": [
    "# Zakupki website scraping for Piotr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfVriCvQPi8u"
   },
   "source": [
    "The aim of this notebook is to scrape details of each contract hosted on the Russian Zakupki public sector contract awarding website.\n",
    "\n",
    "The input for this project will be the Zakupki URL. This code can be run on different dates to pull fresh contract data.\n",
    "\n",
    "Method:\n",
    "1.   Identify the number of pages of contracts to be scraped (using the contract filters provided).\n",
    "2.   Iterate through each page, scraping the registration number of each contract.\n",
    "3.   Access the website for each contract by placing the registraion number in the URL.\n",
    "4.   Scrape the details for each contract and add them to a list of Contracts dataclasses.\n",
    "5.   Format these Contract objects as a dataframe and output the dataframe to a csv file.\n",
    "\n",
    "\n",
    "The output of this project will be the CSV file, with each row representing a new contract from the webstie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O34U-48RLCa"
   },
   "source": [
    "### Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EkywDpTl3_lN"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from urllib3.util import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "from threading import Thread\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import logging\n",
    "import http.client\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import math\n",
    "from os import walk\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the memory leak\n",
    "\n",
    "from collections import Counter\n",
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ohoWQEyyCpkk"
   },
   "outputs": [],
   "source": [
    "logging = False\n",
    "\n",
    "if logging:\n",
    "\n",
    "    http.client.HTTPConnection.debuglevel = 1\n",
    "\n",
    "    # You must initialize logging, otherwise you'll not see debug output.\n",
    "    logging.basicConfig()\n",
    "    logging.getLogger().setLevel(logging.DEBUG)\n",
    "    requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
    "    requests_log.setLevel(logging.DEBUG)\n",
    "    requests_log.propagate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJWBR30Apf9y"
   },
   "source": [
    "### Section 2: Determine Number of pages to scrape\n",
    "Test connection to the website and determine number of pages to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "IxF2lQm5Vq_8"
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# @lru_cache(maxsize=None)\n",
    "def getPage(tempURL):\n",
    "  # If User-Agent is not set to custom, the website will know a Python script is accessing it and block some of the request\n",
    "\n",
    "  response = session.get(tempURL, headers={'User-Agent': 'Custom'})\n",
    "  return BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqetG6kV0HFi",
    "outputId": "d351bc31-6ef2-4d6c-89a8-bf9234b503dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 dates\n"
     ]
    }
   ],
   "source": [
    "# Getting the dates we want to scrape.\n",
    "\n",
    "#url=\"https://zakupki.gov.ru/epz/contractfz223/search/results.html?morphology=on&sortDirection=false&recordsPerPage=_50&showLotsInfoHidden=false&statuses_0=on&statuses_1=on&statuses=0%2C1&priceFrom=1000000&currencyId=-1&contract223DateFrom={}&contract223DateTo={}&sortBy=BY_UPDATE_DATE&pageNumber={}&customerPlace=5277383\"\n",
    "url=\"https://zakupki.gov.ru/epz/order/extendedsearch/results.html?morphology=on&sortBy=UPDATE_DATE&sortDirection=false&recordsPerPage=_500&showLotsInfoHidden=false&fz223=on&pc=on&priceContractAdvantages44IdNameHidden=%7B%7D&priceContractAdvantages94IdNameHidden=%7B%7D&priceFromGeneral=1000000&priceFromGWS=%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%D1%86%D0%B5%D0%BD%D0%B0&priceFromUnitGWS=%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%D1%86%D0%B5%D0%BD%D0%B0&priceToGWS=%D0%9C%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%D1%86%D0%B5%D0%BD%D0%B0&priceToUnitGWS=%D0%9C%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%D1%86%D0%B5%D0%BD%D0%B0&currencyIdGeneral=-1&publishDateFrom={}&publishDateTo={}&pageNumber={}&customerPlace=5277383&selectedSubjectsIdNameHidden=%7B%7D&okdpGroupIdsIdNameHidden=%7B%7D&koksIdsIdNameHidden=%7B%7D&OrderPlacementSmallBusinessSubject=on&OrderPlacementRnpData=on&OrderPlacementExecutionRequirement=on&orderPlacement94_0=0&orderPlacement94_1=0&orderPlacement94_2=0&contractPriceCurrencyId=-1&budgetLevelIdNameHidden=%7B%7D&nonBudgetTypesIdNameHidden=%7B%7D\"\n",
    "startDate = date(2016, 1, 1)\n",
    "endDate = date(2016, 1, 8)\n",
    "days = timedelta(days=1)\n",
    "\n",
    "startDateBefore = startDate\n",
    "\n",
    "calendar=[]\n",
    "\n",
    "while startDate<=endDate:\n",
    "  calendar.append(startDate.strftime('%d.%m.%Y'))\n",
    "  startDate+=days\n",
    "\n",
    "print(\"Created {} dates\".format(len(calendar)))\n",
    "# startDate = date(2016, 1, 1)\n",
    "# endDate = date(2016, 1, 31)\n",
    "# days = timedelta(days=1)\n",
    "\n",
    "# startDateBefore = startDate\n",
    "\n",
    "# calendar=[]\n",
    "\n",
    "# while startDate<=endDate:\n",
    "#   calendar.append(startDate.strftime('%d.%m.%Y'))\n",
    "#   startDate+=days\n",
    "\n",
    "# print(\"Created {} dates\".format(len(calendar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-Ol6Rbipi4N"
   },
   "source": [
    "\n",
    "### Section 3: Scrape each registration number\n",
    "\n",
    "Scrape the reg numbers of each contract, so they can be accessed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page:\n",
    "\n",
    "    def __init__(self, day, pageNum, pagefile):\n",
    "\n",
    "        self.day = day\n",
    "        self.pageNum = pageNum\n",
    "        self.pagefile = pagefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMAKE FOR TENDERS, NOT CONTRACT REGISTRY FOR MORE DATA ###\n",
    "\n",
    "def getContracts(page):\n",
    "# url=\"https://zakupki.gov.ru/epz/order/extendedsearch/results.html?morphology=on&search-filter=%D0%94%D0%B0%D1%82%D0%B5+%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F&pageNumber=1&sortDirection=false&recordsPerPage=_500&showLotsInfoHidden=false&sortBy=UPDATE_DATE&fz223=on&pc=on&priceFromGeneral=1000000&currencyIdGeneral=-1&publishDateFrom=01.01.2016&publishDateTo=08.01.2016&customerPlace=5277383&customerPlaceCodes=66000000000&OrderPlacementSmallBusinessSubject=on&OrderPlacementRnpData=on&OrderPlacementExecutionRequirement=on&orderPlacement94_0=0&orderPlacement94_1=0&orderPlacement94_2=0\"\n",
    "# soup=getPage(url)\n",
    "\n",
    " \n",
    "    # Obtain a list of all the sections of HTML containing a contract in the web page\n",
    "    listOfContracts = page.find_all(\"div\", {\"class\": \"registry-entry__header-mid__number\"})\n",
    "    regNumbersList=[]\n",
    "\n",
    "    for contract in listOfContracts:\n",
    "                regNum = contract.find(\"a\")['href'].split(\"Id=\")[1] ### changed from =id?\n",
    "                regNumbersList.append(regNum)\n",
    "                \n",
    "    return regNumbersList\n",
    "# print(regNumbersList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(idx, data):\n",
    "\n",
    "    x_ = int(((idx+1) * 100) / len(data))\n",
    "    y_ = idx % math.ceil(len(data) / 10)\n",
    "    \n",
    "    print(\" ----\\n{}% completed\\n----\".format(x_)) if y_ == 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "seJyyDBCFLGl"
   },
   "outputs": [],
   "source": [
    "# Getting the web page for all the contracts for each date in the range we want to scrape.\n",
    "\n",
    "regNumbersDict = {}\n",
    "\n",
    "def getRegNumbersForDate(i, day):\n",
    "\n",
    "  if day in regNumbersDict:\n",
    "    return\n",
    "\n",
    "  tempURL = url.format(day, day, 1)\n",
    "\n",
    "  print(tempURL)\n",
    "\n",
    "  page = getPage(tempURL)\n",
    "\n",
    "  # Scrape the max number of pages\n",
    "  try:\n",
    "    maxPageNum = int(page.select('a[data-pagenumber]')[-2].find(\"span\").text)\n",
    "    print(\"{} pages for this day\".format(maxPageNum))\n",
    "  except:\n",
    "    maxPageNum = 1\n",
    "\n",
    "\n",
    "  # Leave my variable names alone :(\n",
    "  totalRegNumbersForThisDay = 0\n",
    "\n",
    "  for i in range(1, maxPageNum+1):\n",
    "\n",
    "    # Creating a temporary URL for each page containing contracts\n",
    "    tempPageURL = url.format(day, day, i)\n",
    "\n",
    "    # Request the page and format it as a BeautifulSoup object so that we can perform scrapings\n",
    "    page = getPage(tempPageURL)\n",
    "\n",
    "    regNumbersList = getContracts(page)\n",
    "\n",
    "    totalRegNumbersForThisDay += len(regNumbersList)\n",
    "\n",
    "    regNumbersDict[day] = regNumbersList\n",
    "\n",
    "\n",
    "  print(\"Fetched day {} had {} contracts \\n\".format(day, totalRegNumbersForThisDay), end='')\n",
    "\n",
    "  del page\n",
    "  del regNumbersList\n",
    "\n",
    "  progress(i, calendar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached day 01.01.2016 had 0 contracts \n",
      "Cached day 02.01.2016 had 0 contracts \n",
      "Cached day 03.01.2016 had 0 contracts \n",
      "Cached day 04.01.2016 had 1 contracts \n",
      "Cached day 05.01.2016 had 4 contracts \n",
      "Cached day 06.01.2016 had 7 contracts \n",
      "Cached day 07.01.2016 had 0 contracts \n",
      "Cached day 08.01.2016 had 1 contracts \n",
      "{'10.01.2016': [], '12.01.2016': ['1138537', '3005868', '1631882', '1381830', '2763714', '1134959', '3239957', '1631938', '1631921', '1789962', '1115200', '1136496', '2817997', '1711845', '1336972', '1135059', '1144111', '1776557', '1172771', '1137429', '1177497', '1390129', '1143471', '1230260', '1223392', '1212212', '1191865', '1144382', '1144117', '1138077', '1130704', '1136136', '1124899', '1118980'], '11.01.2016': [], '04.01.2016': ['3802335'], '07.01.2016': [], '03.01.2016': [], '08.01.2016': ['3803745'], '01.01.2016': [], '02.01.2016': [], '05.01.2016': ['3802625', '3801801', '3802782', '3802861'], '06.01.2016': ['3803312', '3803050', '3803069', '3812293', '3803501', '3803171', '3803152']}\n",
      "['3802335', '3802625', '3801801', '3802782', '3802861', '3803312', '3803050', '3803069', '3812293', '3803501', '3803171', '3803152', '3803745']\n",
      "------------------- \n",
      " 13 contracts found in total\n"
     ]
    }
   ],
   "source": [
    "### This part doesn't work, script is not going through pages, scrapes only first e.g. out of 3, \n",
    "### and doesn't save contract regnumbers \n",
    "## This now has regNumbers caching too \n",
    "\n",
    "cachedRegNums223 = {}\n",
    "\n",
    "# load the data from the json file\n",
    "with open('cachedRegNums223.json', 'r') as f:\n",
    "  cachedRegNums223 = json.load(f)\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=50) as ex:\n",
    "  for i, day in enumerate(calendar):\n",
    "    if day in cachedRegNums223:\n",
    "      regNumbersDict[day] = cachedRegNums223[day]\n",
    "      print(\"Cached day {} had {} contracts \\n\".format(day, len(regNumbersDict[day])), end='')\n",
    "    else:\n",
    "      ex.submit(getRegNumbersForDate, i, day)\n",
    "\n",
    "    \n",
    "combinedRegNumbersDict = {**cachedRegNums223, **regNumbersDict}\n",
    "\n",
    "print(combinedRegNumbersDict)\n",
    "\n",
    "\n",
    "with open('cachedRegNums223.json', 'w') as f:\n",
    "  json.dump(combinedRegNumbersDict, f)\n",
    "\n",
    "\n",
    "tempRegNumbers = list(regNumbersDict.values())\n",
    "\n",
    "regNumbers = []\n",
    "\n",
    "for t in tempRegNumbers:\n",
    "  regNumbers.extend(t)\n",
    "\n",
    "print(regNumbers)\n",
    "\n",
    "print(\"------------------- \\n {} contracts found in total\".format(len(regNumbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L254l5YhpqNv"
   },
   "source": [
    "### Section 4: Details scraping\n",
    "\n",
    "The Contract Dataclass will store the information during scraping.\n",
    "If any information can't be scraped, default values have been provided in their place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Eg5Td-OGi3vz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Contract:\n",
    "\n",
    "  # TODO: Add reg number to class\n",
    "\n",
    "  # Main Section\n",
    "  id: float = 0\n",
    "  price: float = 0.0\n",
    "  signed: date = None\n",
    "  # deadline: date = None\n",
    "\n",
    "  # Tab 1\n",
    "  method: str = \"none\"\n",
    "  procurer: str = \"none\"\n",
    "  # supplier: str = \"none\"\n",
    "  proinn: str = \"none\"\n",
    "  # supinn: str = \"none\"\n",
    "  # registered: date = None\n",
    "  person: str= \"none\"\n",
    "  address: str = \"none\"\n",
    "  number: str = \"none\"\n",
    "  mail: str = \"none\"\n",
    "\n",
    "  # Tab 2\n",
    "  code: float = 0.0\n",
    "  product: str = \"none\"\n",
    "  \n",
    "\n",
    "  def __repr__(self):\n",
    "    return \"\\nContract id= {} \\n First tab: price={}, signed={}, method={}, procurer={}, proinn={}, person={}, address={}, number={}, mail={}, product={} \\n  Second tab: code={})\".format(self.id, self.price, self.signed, self.method, self.procurer, self.proinn, self.person, self.address, self.number, self.mail, self.product, self.code)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yatvXkvpz2H"
   },
   "source": [
    "Method for scraping the data from each contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSectionDict(page):\n",
    "\n",
    "    sections=page.findAll(\"div\",{\"class\":\"col-9 mr-auto\"})\n",
    "\n",
    "    # print([key.findAll(\"span\") for key in sections])\n",
    "\n",
    "    # Turning the sections into a dictionary that will be easier to work with.\n",
    "    pairs = [key.findAll(\"div\") for key in sections]\n",
    "\n",
    "    pairs = list(filter(None, pairs))\n",
    "\n",
    "\n",
    "    titles = []\n",
    "    values = []\n",
    "\n",
    "    for x in pairs:\n",
    "        if len(x) > 1:\n",
    "            try:\n",
    "                titles.append(x[0])\n",
    "                values.append(x[1])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    sectionDict = {titles[i].text.strip() : values[i].text.strip() for i in range(len(titles))}\n",
    "\n",
    "    return sectionDict\n",
    "\n",
    "\n",
    "# def getTableDict(page, secondTab=False):\n",
    "\n",
    "\n",
    "#     if secondTab:\n",
    "#         sectionOfInterest = page.findAll(\"div\", {\"class\": \"col\"})[-1]\n",
    "#     else:\n",
    "#         sectionOfInterest = page\n",
    "\n",
    "#     table = sectionOfInterest.findAll(\"tr\",{\"class\":\"tableBlock__row\"})\n",
    "\n",
    "#     # print(table[3])\n",
    "\n",
    "#     headers = [i.text.strip() for i in table[0].findAll(\"th\", {\"class\":\"tableBlock__col tableBlock__col_header\"})]\n",
    "#     data = [list(filter(None, [j.strip() for j in i.text.split(\"\\n\")])) for i in table[1].findAll(\"td\")]\n",
    "\n",
    "#     if len(headers) == 0:\n",
    "#         headers = [i.text.strip() for i in table[2].findAll(\"th\", {\"class\":\"tableBlock__col tableBlock__col_header\"})]\n",
    "#         data = [list(filter(None, [j.strip() for j in i.text.split(\"\\n\")])) for i in table[3].findAll(\"td\")]\n",
    "\n",
    "#     if len(headers) == 0:\n",
    "#         headers = [i.text.strip() for i in table[3].findAll(\"th\", {\"class\":\"tableBlock__col tableBlock__col_header\"})]\n",
    "#         data = [list(filter(None, [j.strip() for j in i.text.split(\"\\n\")])) for i in table[4].findAll(\"td\")]\n",
    "\n",
    "\n",
    "\n",
    "#     # This is hacky.\n",
    "#     if len(data) < len(headers):\n",
    "#         data = [[[]] for i in range(len(headers))]\n",
    "\n",
    "        \n",
    "\n",
    "#     tableDict = {headers[i] : data[i] for i in range(len(headers))}\n",
    "\n",
    "#     return tableDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Jq9BLFkUaDPW"
   },
   "outputs": [],
   "source": [
    "def scrapeData(reg):\n",
    "\n",
    "  try:\n",
    "    # Input: reg = one registration number.\n",
    "\n",
    "    # Different URL from the one above, this accesses more information from Zakupki.\n",
    "    dir=\"https://zakupki.gov.ru/epz/cnotice/notice223/{}.html?noticeInfoId={}\"\n",
    "    #\"https://zakupki.gov.ru/epz/contract/contractCard/{}.html?reestrNumber={}\"\n",
    "\n",
    "    # Getting the web page for the given contract\n",
    "    tempDir = dir.format(\"common-info\", reg)\n",
    "    page = getPage(tempDir)\n",
    "\n",
    "    # We probably don't need this with the method I've used below.\n",
    "    # contractTypeTwo = False\n",
    "\n",
    "    # Enter the text here that should be present to signify the second type of contract.\n",
    "    # if page.findAll(text=\"Основание заключения контракта с единственным поставщиком\"):\n",
    "    #   contractTypeTwo = True\n",
    "      \n",
    "    id = reg  \n",
    "    sectionDict = getSectionDict(page)\n",
    "    # firstTableDict = getTableDict(page)\n",
    "\n",
    "    # print(sectionDict)\n",
    "    # print(firstTableDict)\n",
    "\n",
    "    # print(sectionDict, firstTableDict)\n",
    "    # try:\n",
    "    #   price=sectionDict[\"Цена контракта\"].replace(\"\\xa0\",\"\").replace(\",\",\".\").replace(\"₽\",\"\").strip().split()[0]\n",
    "    # except:\n",
    "    #   price=sectionDict[\"Ориентировочное значение цены контракта\"].replace(\"\\xa0\",\"\").replace(\",\",\".\").replace(\"₽\",\"\").strip().split()[0]\n",
    "    #   try:\n",
    "    #     price=sectionDict[\"Максимальное значение цены контракта\"].replace(\"\\xa0\",\"\").replace(\",\",\".\").replace(\"₽\",\"\").strip().split()[0]\n",
    "    #   except:\n",
    "    \n",
    "    price=page.find('div', {'class':'price-block__value'}).text.strip().replace(\"₽\",\"\").replace(\",\",\".\").replace(\" \", \"\")\n",
    "    signed=page.find('div',{'class':'data-block__value'}).text.strip()\n",
    "    # deadline=sectionDict[\"Дата окончания исполнения контракта\"].split()[0]\n",
    "    \n",
    "    ### fixed issue with method ### \n",
    "    try:\n",
    "      method = sectionDict[\"Способ осуществления закупки\"]\n",
    "    except:\n",
    "      if page.findAll(text=\"Основание заключения контракта с единственным поставщиком\"):\n",
    "            method=\"Закупка у единственного поставщика (подрядчика, исполнителя)\"\n",
    "            \n",
    "    procurer=sectionDict[\"Наименование организации\"]\n",
    "    # supplier=firstTableDict[\"Организация\"][0]\n",
    "\n",
    "    proinn=page.find('div', {'class':'ml-1 common-text__value'}).text.strip()\n",
    "\n",
    "    ### fixed issues for missing values sometimes in the table ###\n",
    "    \n",
    "    # registered=firstTableDict[\"Организация\"][-1]\n",
    "    \n",
    "    ### fixed, testing ###   \n",
    "    # try: \n",
    "    #   if firstTableDict[\"Организация\"][-4]==\"КПП:\":\n",
    "    #       supinn=firstTableDict[\"Организация\"][-5]\n",
    "    #   else:\n",
    "    #       supinn=firstTableDict[\"Организация\"][-3]\n",
    "    # except:\n",
    "    #   supinn=\"\"\n",
    "\n",
    "    ### fixed issues in lower table ### \n",
    "    address=sectionDict['Место нахождения']\n",
    "    mail=sectionDict['Адрес электронной почты']\n",
    "    number=sectionDict['Контактный телефон']\n",
    "    product=sectionDict['Наименование закупки']\n",
    "    person=sectionDict['Контактное лицо']\n",
    "\n",
    "\n",
    "    ### details about winner - ALSO, THERE'S OPTION TO SCRAPE SUBCONTRACTORS ### \n",
    "\n",
    "    page.decompose()\n",
    "\n",
    "    ### Second tab ###\n",
    "    tempDir = dir.format(\"lot-list\", reg)\n",
    "    page = getPage(tempDir)\n",
    "\n",
    "    ### code stands for the product code, which can be later identified to return industry type ### \n",
    "    \n",
    "    # secondTableDict = getTableDict(page, True)\n",
    "    \n",
    "    codetablevalues=page.findAll({\"td\":\"class\"})\n",
    "    \n",
    "    try:\n",
    "      code = code=codetablevalues[3].text.strip()\n",
    "    except:\n",
    "      code = ''\n",
    "        \n",
    "    # product = secondTableDict[\"Наименование объекта закупки и его характеристики\"][0]\n",
    "    \n",
    "    # Create the Contract dataclass object and append it to a list of objects.\n",
    "    # This method means that missing data can be accounted for.\n",
    "    # print(method)\n",
    "\n",
    "    contract = Contract(id=id, price=price, signed=signed, method=method, procurer=procurer, proinn=proinn, address=address, person=person, number=number, mail=mail, code=code, product=product)\n",
    "    \n",
    "    # contracts.append(contract)\n",
    "    # print('Completed {}'.format(id))\n",
    "\n",
    "    page.decompose()\n",
    "\n",
    "    return contract\n",
    "  except Exception as e:\n",
    "    failedRegNumbers.append(reg)\n",
    "    print(\"Failed to scrape {}\".format(reg))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CitfSwF2p94x"
   },
   "source": [
    "### Section 5: Starting execution\n",
    "Scrape the contracts themselves using threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(reg):\n",
    "    \n",
    "    try:\n",
    "        _ = int(reg)\n",
    "        # print(\"Scraping {}\".format(reg))\n",
    "        return [scrapeData(reg)]\n",
    "    except TypeError:\n",
    "        \n",
    "        # TODO make 500 contracts change here.\n",
    "        \n",
    "        contracts = []\n",
    "\n",
    "        \n",
    "\n",
    "        for idx, r in enumerate(reg):\n",
    "            # print(\"Scraping {}\".format(r))\n",
    "            contracts.append(scrapeData(r))\n",
    "            # progress(i + idx, regNumbers)\n",
    "\n",
    "        return contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RseKcK3_XT7R",
    "outputId": "2e193036-25a2-4b02-8d91-34a6293d9141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape with 13 reg numbers\n",
      "\n",
      "13 of 13 contracts are uncached. Fetching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:51, 51.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803152\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:52, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3802625\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n",
      "Failed to scrape 3802782\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:52,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803050\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n",
      "Failed to scrape 3812293\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:53,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803745\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:53,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803312\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n",
      "Failed to scrape 3802335\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:53,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3802861\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:54,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803501\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:55,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803171\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:56,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape 3803069\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n",
      "Failed to scrape 3801801\n",
      "'NoneType' object has no attribute 'text'\n",
      "'NoneType' object has no attribute '__dict__'\n",
      "Failed to scrape 13 contracts\n",
      "['3803152', '3802625', '3802782', '3803050', '3812293', '3803745', '3803312', '3802335', '3802861', '3803501', '3803171', '3803069', '3801801']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 lines\n",
      "#1: core\\completer.py:2269: 11198.5 KiB\n",
      "    names.append(unicodedata.name(chr(c)))\n",
      "#2: <frozen importlib._bootstrap_external>:672: 172.4 KiB\n",
      "#3: lib\\tracemalloc.py:558: 144.5 KiB\n",
      "    traces = _get_traces()\n",
      "2108 other: 2168.9 KiB\n",
      "Total allocated size: 13684.3 KiB\n"
     ]
    }
   ],
   "source": [
    "progressNum = 0\n",
    "\n",
    "failedRegNumbers = []\n",
    "\n",
    "# regNumbers = [14511297]\n",
    "\n",
    "print(\"Starting scrape with {} reg numbers\\n\".format(len(regNumbers)))\n",
    "\n",
    "# scrape(regNumbers[:10])\n",
    "\n",
    "# for contract in contracts:\n",
    "#     print(contract)\n",
    "\n",
    "# for regNumber in tqdm(regNumbers[:50]):\n",
    "#   thread = Thread(target = scrapeData, args = (regNumber,))\n",
    "#   thread.start()\n",
    "\n",
    "# regNumbers = ['3662502457421000001']\n",
    "# regNumbers = ['1665800691921000016']\n",
    "\n",
    "threading = True\n",
    "\n",
    "if threading:\n",
    "\n",
    "    interval = 1\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as ex:\n",
    "        threads = []\n",
    "\n",
    "        cachedContracts = {}\n",
    "\n",
    "        with open('cachedContracts.csv', encoding=\"utf-8\") as f:\n",
    "            cachedContracts = list(csv.reader(f))\n",
    "        \n",
    "        cachedContracts = list(filter(None, cachedContracts))\n",
    "        \n",
    "        cachedContractRegNums = [row[0] for row in cachedContracts]\n",
    "\n",
    "        uncachedRegNumbers = list(set(regNumbers) - set(cachedContractRegNums))\n",
    "\n",
    "        print(\"{} of {} contracts are uncached. Fetching...\".format(len(uncachedRegNumbers), len(regNumbers)))\n",
    "        \n",
    "        for i in range(0, len(uncachedRegNumbers), interval):\n",
    "            tempNumbers = uncachedRegNumbers[i:i+interval]\n",
    "            # print(tempNumbers)\n",
    "            threads.append(ex.submit(scrape, tempNumbers))\n",
    "        \n",
    "\n",
    "        completed = 0\n",
    "\n",
    "        for result in tqdm(as_completed(threads)):\n",
    "\n",
    "            try:\n",
    "                contracts = result.result()\n",
    "\n",
    "                completed += interval\n",
    "\n",
    "                # progress(completed, uncachedRegNumbers)\n",
    "\n",
    "                formattedContracts = [list(contract.__dict__.values()) for contract in contracts]\n",
    "\n",
    "                # newCachedContracts = {**cachedContracts, **formattedContracts}\n",
    "\n",
    "                with open('cachedContracts.csv', 'a', encoding=\"utf-8\", newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerows(formattedContracts)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "            # print(\"{} finished\".format(interval))\n",
    "\n",
    "else:\n",
    "\n",
    "    scrape(regNumbers)\n",
    "\n",
    "# print(\"Scraped {} contracts\".format(len(contracts)))\n",
    "print(\"Failed to scrape {} contracts\".format(len(failedRegNumbers)))\n",
    "print(failedRegNumbers) if len(failedRegNumbers) > 0 else None\n",
    "\n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "display_top(snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the data is saved to the hard disk, we can run the below code without needing to rerun the scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 2016-01-08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>signed</th>\n",
       "      <th>method</th>\n",
       "      <th>procurer</th>\n",
       "      <th>proinn</th>\n",
       "      <th>person</th>\n",
       "      <th>address</th>\n",
       "      <th>number</th>\n",
       "      <th>mail</th>\n",
       "      <th>code</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, price, signed, method, procurer, proinn, person, address, number, mail, code, product]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedContracts = []\n",
    "\n",
    "with open('cachedContracts.csv', encoding=\"utf-8\") as f:\n",
    "    cachedContracts = list(csv.reader(f))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=Contract().__dict__.keys(), data=cachedContracts)\n",
    "\n",
    "df['signed'] = pd.to_datetime(df['signed'])\n",
    "# df['deadline'] = pd.to_datetime(df['deadline'])\n",
    "\n",
    "start = datetime.fromordinal(startDateBefore.toordinal()).strftime(\"%Y-%m-%d\")\n",
    "end = datetime.fromordinal(endDate.toordinal()).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(start, end)\n",
    "\n",
    "mask = (df['signed'] >= start) & (df['signed'] <= end)\n",
    "\n",
    "selectedDatesDF = df.loc[mask]\n",
    "\n",
    "selectedDatesDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 12)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedDatesDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9_G7hUrqT6g"
   },
   "source": [
    "### Section 6: Output\n",
    "\n",
    "Convert the list of contract classes to a dataframe so that they can be exported to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "9ST7Qk4rVOFK"
   },
   "outputs": [],
   "source": [
    "selectedDatesDF.to_csv(\"zakupki{}to{}.csv\".format(startDateBefore, endDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Zakupki.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "77edd9a980bfddd358a4fcbf4f3e7f49f98b43dd432ee51433ccfac9d125dab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
